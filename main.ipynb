{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_lines\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "REMOVEABLE_CHARS = \"»<>123456789\\/&()?:!.,;'´\"\n",
    "# ALL_TAGS = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT',\n",
    "#  'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP',\n",
    "#  'WP$', 'WRB']\n",
    "ACCEPTABLE_TAGS = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT',\n",
    " 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP',\n",
    " 'WP$', 'WRB']\n",
    "MAX_FEATURES = 10\n",
    "MIN_ENGLISH = 0.9999\n",
    "LIMIT_INPUT_ROWS = 100 # to stop run times hindering development\n",
    "\n",
    "def get_wordnet_pos(tag): # Return a char, based on input tag, that is used as a param in lemmatisation for enhanced results\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN INFO\n",
    "X= []; y= []; z= [];\n",
    "with open ('reviews.json', 'rb') as f:\n",
    "    for i, item in enumerate(json_lines.reader(f)):\n",
    "        if i <= LIMIT_INPUT_ROWS:\n",
    "            try:\n",
    "                langs = detect_langs(item['text'])\n",
    "            except:\n",
    "                pass\n",
    "            for i, lang in enumerate(langs):\n",
    "                if str(lang)[0:2] == 'en':\n",
    "                    if float(str(lang)[3:]) > MIN_ENGLISH:\n",
    "                        X.append(item['text'])\n",
    "                        y.append(item['voted_up'])\n",
    "                        z.append(item['early_access'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i, item in enumerate(X):\n",
    "#     print( \"X[\", str(i), \"]: \", str(X[i]) )\n",
    "#     print( \"y[\", str(i), \"]: \", str(y[i]) )\n",
    "#     print( \"z[\", str(i), \"]: \", str(z[i]) )\n",
    "#     print( \"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER TEXT\n",
    "for i, text in enumerate(X):\n",
    "    text_words = nltk.word_tokenize(text.lower())\n",
    "    tags = nltk.pos_tag(text_words)\n",
    "    #print(tags)\n",
    "    new_text= [];\n",
    "    for j, word in enumerate(text_words):\n",
    "        if tags[j][1] in ACCEPTABLE_TAGS:\n",
    "            for char in word:\n",
    "                if char in REMOVEABLE_CHARS:\n",
    "                    break\n",
    "            else:\n",
    "                wordnet_pos = get_wordnet_pos(tags[j][1])\n",
    "                if wordnet_pos != '':\n",
    "                    new_text.append(lemmatizer.lemmatize(word, wordnet_pos))\n",
    "                    #print(\"word: \", word, \". wordnet_pos: \", wordnet_pos, \"lemmad: \",lemmatizer.lemmatize(word, wordnet_pos),\"\\n\")\n",
    "                continue\n",
    "    X[i] = new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, item in enumerate(X):\n",
    "#     print( \"X[\", str(i), \"]: \", str(X[i]) )\n",
    "#     print( \"y[\", str(i), \"]: \", str(y[i]) )\n",
    "#     print( \"z[\", str(i), \"]: \", str(z[i]) )\n",
    "#     print( \"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'be': 42, 'game': 33, 'i': 22, 'have': 13, 'not': 10, 'play': 9, 'get': 9, 'much': 8, 'do': 8, 'friend': 8}\n"
     ]
    }
   ],
   "source": [
    "# MOST FREQ WORDS AS FEATURES\n",
    "word_counts = {};\n",
    "for text in X:\n",
    "    for word in text:\n",
    "        if word in word_counts:\n",
    "            word_counts[word]+= 1\n",
    "        else:\n",
    "            word_counts[word]= 1\n",
    "word_counts = {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "trimd_word_counts = {}\n",
    "if len(word_counts) > MAX_FEATURES:\n",
    "    for i, entry in enumerate(word_counts):\n",
    "        if i >= MAX_FEATURES:\n",
    "            break\n",
    "        trimd_word_counts[entry] = word_counts[entry]\n",
    "word_counts = trimd_word_counts\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CONVERT TO PANDAS\n",
    "# d = {'Text':X, 'Voted up':y, 'Early access':z}\n",
    "# df = pd.DataFrame(d)\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
